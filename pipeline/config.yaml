# Fine-tuning Pipeline Configuration
# Align with frontend: src/config/prompts.ts, src/services/ai/contextProvider.ts

person_name: "Justin"
person_full_name: "Justin Law"

# Paths
resume_path: "resume/resume.pdf"
dataset_output: "data"
model_output: "models/lora"
merged_output: "models/merged"
onnx_output: "models/onnx"

# llama-server for dataset generation
server:
  host: "http://localhost"
  port: 8080
  timeout: 120
  health_check_timeout: 5

validation:
  context_token_warn: 2000

generation_limits:
  question_max_tokens: 64
  answer_max_tokens: 80
  min_question_length: 15
  min_answer_length: 15
  min_variation_length: 15
  max_question_words: 15
  max_answer_sentences: 2
  max_answer_words: 50

llm_defaults:
  temperature: 0.7
  max_tokens: 128
  variation_max_tokens: 64
  long_question_skip_prob: 0.1
  stop_sequences:
    - "\n\n\n"
    - "Question:"

# Dataset Generation
dataset:
  hub_id: "justinthelaw/Resume-Cover-Letter-SFT-Dataset"
  samples_per_category: 2000
  include_military: true
  has_recommendations: true
  variations_per_question: 3
  temperatures:
    question: 0.9
    answer: 0.1
    variation: 1.0
  train_split: 0.9
  seed: 88

# Base Model
model:
  base: "HuggingFaceTB/SmolLM2-360M-Instruct"
  hub_id: "justinthelaw/SmolLM2-360M-Instruct-Resume-Cover-Letter-SFT"

# LoRA Config
lora:
  r: 128
  alpha: 256
  dropout: 0.05
  target_modules:
    - q_proj
    - k_proj
    - v_proj
    - o_proj
    - gate_proj
    - up_proj
    - down_proj

# SFT Training with LoRA
sft:
  epochs: 16
  batch_size: 16
  gradient_accumulation: 4
  learning_rate: 2.0e-5
  max_length: 384
  warmup_ratio: 0.05
  seed: 88
  weight_decay: 0.0
  logging_steps: 10
  save_total_limit: 2

inference:
  max_new_tokens: 128
  repetition_penalty: 1.2

quantization:
  block_size: 32
  accuracy_level: 4

evaluation:
  seed: 88
  smoke_samples: 50
  full_samples_per_set: 300
  max_new_tokens: 128
  report_output: "data/eval_reports"
  eval_data_dir: "data/eval"
  refusal_markers:
    - "I don't have enough information"
    - "I do not have enough information"
    - "I can't answer"
    - "I cannot answer"
    - "I don't know"
    - "I do not know"
    - "not in the provided context"
    - "outside the scope"
  thresholds:
    exact_match_rate_min: 0.35
    token_f1_min: 0.55
    keyword_coverage_min: 0.75
    refusal_accuracy_min: 0.90
    response_length_compliance_min: 0.95
    behavior_accuracy_min: 0.92
    fp32_alignment_min: 0.70
    case_token_f1_min: 0.25
    case_keyword_coverage_min: 0.40
    p95_latency_ms_max:
      model.onnx: 2500
      model_int8.onnx: 1800
      model_uint8.onnx: 1800
      model_q4.onnx: 1200
  hub:
    model_id: "justinthelaw/SmolLM2-360M-Instruct-Resume-Cover-Letter-SFT"
    dataset_id: "justinthelaw/Resume-Cover-Letter-SFT-Dataset"
