# Fine-tuning Pipeline Configuration
# Align with frontend: src/config/prompts.ts, src/services/ai/contextProvider.ts

person_name: "Justin"
person_full_name: "Justin Law"

# Paths
resume_path: "resume/resume.pdf"
dataset_output: "data"
model_output: "models/lora"
merged_output: "models/merged"
onnx_output: "models/onnx"

# llama-server for dataset generation
server:
  host: "http://localhost"
  port: 8080
  timeout: 120

# Dataset Generation
dataset:
  hub_id: "justinthelaw/Resume-Cover-Letter-SFT-Dataset"
  samples_per_category: 2000
  include_military: true
  has_recommendations: true
  variations_per_question: 3
  temperatures:
    question: 0.9
    answer: 0.1
    variation: 1.0
  train_split: 0.9
  seed: 88

# Base Model
model:
  base: "HuggingFaceTB/SmolLM2-360M-Instruct"
  hub_id: "justinthelaw/SmolLM2-360M-Instruct-Resume-Cover-Letter-SFT"

# LoRA Config
lora:
  r: 128
  alpha: 256
  dropout: 0.05
  target_modules:
    - q_proj
    - k_proj
    - v_proj
    - o_proj
    - gate_proj
    - up_proj
    - down_proj

# SFT Training with LoRA
sft:
  epochs: 16
  batch_size: 16
  gradient_accumulation: 4
  learning_rate: 2.0e-5
  max_length: 384
  warmup_ratio: 0.05
  seed: 88
  weight_decay: 0.0
